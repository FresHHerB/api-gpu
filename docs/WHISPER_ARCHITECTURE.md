# Whisper Transcription Architecture

## ğŸ“š VisÃ£o Geral

Este documento explica a arquitetura de transcriÃ§Ã£o de Ã¡udio usando RunPod Workers e Hubs.

---

## ğŸ—ï¸ Arquitetura: Orquestrador â†’ Worker â†’ Hub

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚     Cliente     â”‚
â”‚   (HTTP POST)   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ POST /runpod/audio/transcribe
         â”‚ {audio_url, path, model}
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚           ORQUESTRADOR (api-gpu)                   â”‚
â”‚                                                    â”‚
â”‚  1. Valida input (audio_url, path, model)         â”‚
â”‚  2. Chama RunPod Worker (via API)                 â”‚
â”‚  3. Aguarda resposta (polling)                    â”‚
â”‚  4. Recebe output bruto (segments + words)        â”‚
â”‚  5. Formata arquivos (SRT, JSON, ASS karaoke)     â”‚
â”‚  6. Upload para S3/MinIO                          â”‚
â”‚  7. Retorna URLs + estatÃ­sticas                   â”‚
â”‚                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ POST /v2/{endpoint_id}/run
         â”‚ {input: {audio, model, language, ...}}
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚          RUNPOD WORKER (Serverless)                â”‚
â”‚                                                    â”‚
â”‚  1. Recebe job via RunPod handler                 â”‚
â”‚  2. Valida input (audio, model, etc)              â”‚
â”‚  3. Faz download do Ã¡udio (temp file)             â”‚
â”‚  4. Executa transcriÃ§Ã£o com Whisper               â”‚
â”‚  5. Extrai segments + word_timestamps             â”‚
â”‚  6. Retorna output bruto:                         â”‚
â”‚     - segments[]                                  â”‚
â”‚     - word_timestamps[] (opcional)                â”‚
â”‚     - detected_language                           â”‚
â”‚     - transcription (texto completo)              â”‚
â”‚     - device, model                               â”‚
â”‚  7. Cleanup (deleta temp file)                    â”‚
â”‚                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â”‚ Usa imagem Docker do Hub
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚            RUNPOD HUB (Cache Registry)             â”‚
â”‚                                                    â”‚
â”‚  - Armazena imagem Docker prÃ©-built                â”‚
â”‚  - Permite cold start rÃ¡pido (~6s vs 8+ min)      â”‚
â”‚  - Versionamento (v1.0.0, v1.1.0, etc)            â”‚
â”‚  - Testes automÃ¡ticos antes de publicar           â”‚
â”‚                                                    â”‚
â”‚  ConteÃºdo da imagem:                              â”‚
â”‚  - CUDA 11.8 + cuDNN                              â”‚
â”‚  - PyTorch 2.1.2                                  â”‚
â”‚  - OpenAI Whisper (ou faster-whisper)             â”‚
â”‚  - Modelos prÃ©-cacheados (base, medium, turbo)    â”‚
â”‚  - Handler RunPod (src/handler.py)                â”‚
â”‚                                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Fluxo Detalhado

### 1. Cliente â†’ Orquestrador

**Endpoint:** `POST /runpod/audio/transcribe`

**Payload:**
```json
{
  "audio_url": "http://minio.automear.com/canais/.../audio_final.mp3",
  "path": "CANAL/VIDEO/transcriptions/",
  "model": "base"
}
```

**Headers:**
```
X-API-Key: sua-chave-api
Content-Type: application/json
```

---

### 2. Orquestrador â†’ RunPod Worker

**ServiÃ§o:** `RunPodWhisperService`

**AÃ§Ãµes:**
1. ConstrÃ³i payload RunPod
2. Submete job: `POST https://api.runpod.ai/v2/{endpoint_id}/run`
3. Recebe `job_id`
4. Polling: `GET https://api.runpod.ai/v2/{endpoint_id}/status/{job_id}`
5. Aguarda status `COMPLETED`

**Payload enviado ao worker:**
```json
{
  "input": {
    "audio": "http://minio.automear.com/.../audio.mp3",
    "model": "base",
    "transcription": "plain_text",
    "translate": false,
    "language": null,
    "temperature": 0,
    "best_of": 5,
    "beam_size": 5,
    "patience": 1,
    "suppress_tokens": "-1",
    "condition_on_previous_text": true,
    "temperature_increment_on_fallback": 0.2,
    "compression_ratio_threshold": 2.4,
    "logprob_threshold": -1.0,
    "no_speech_threshold": 0.6,
    "enable_vad": true,
    "word_timestamps": true
  }
}
```

---

### 3. Worker Processamento

**Handler:** `src/handler.py` â†’ `run_whisper_job()`

**Fluxo:**
1. Extrai `job.input.audio`, `job.input.model`, etc
2. Chama `download_audio(audio_url)` â†’ salva em `/tmp/audio_xyz.mp3`
3. Chama `MODEL.predict(audio_path, model, language, ...)`
4. `predict()` carrega modelo Whisper (cache se jÃ¡ carregado)
5. Executa `whisper_model.transcribe(audio_path, ...)`
6. Extrai `segments` e `word_timestamps`
7. Retorna output
8. Deleta arquivo temporÃ¡rio

**Output retornado ao orquestrador:**
```json
{
  "segments": [
    {
      "id": 0,
      "seek": 0,
      "start": 0.0,
      "end": 2.5,
      "text": "God is unfolding His plan...",
      "tokens": [1234, 5678, ...],
      "temperature": 0.0,
      "avg_logprob": -0.234,
      "compression_ratio": 1.8,
      "no_speech_prob": 0.01
    },
    ...
  ],
  "word_timestamps": [
    {"word": "God", "start": 0.0, "end": 0.3},
    {"word": "is", "start": 0.35, "end": 0.5},
    ...
  ],
  "detected_language": "en",
  "transcription": "God is unfolding His plan for you...",
  "device": "cuda",
  "model": "base"
}
```

---

### 4. Orquestrador PÃ³s-Processamento

**ServiÃ§os:**
- `TranscriptionFormatter`: Formata output em diferentes formatos
- `S3UploadService`: Upload para MinIO/S3

**Fluxo:**
1. Recebe output bruto do worker
2. Gera `segments.srt` via `TranscriptionFormatter.toSRT(segments)`
3. Gera `words.json` via `TranscriptionFormatter.toJSON(words)`
4. Gera `karaoke.ass` via `TranscriptionFormatter.toASSKaraoke(words)` (se word_timestamps disponÃ­vel)
5. Upload para S3:
   - `{path}/segments.srt`
   - `{path}/words.json`
   - `{path}/karaoke.ass`
6. Retorna URLs pÃºblicas + estatÃ­sticas

**Response ao cliente:**
```json
{
  "code": 200,
  "message": "Transcription completed successfully",
  "job_id": "f3dc61b8-60d5-44ea-b359-ea4d8bd7c280",
  "language": "en",
  "transcription": "God is unfolding His plan for you...",
  "files": {
    "segments": {
      "srt": "https://minio.automear.com/.../segments.srt",
      "vtt": "",
      "json": "https://minio.automear.com/.../words.json"
    },
    "words": {
      "ass_karaoke": "https://minio.automear.com/.../karaoke.ass",
      "vtt_karaoke": "",
      "lrc": "",
      "json": "https://minio.automear.com/.../words.json"
    }
  },
  "execution": {
    "startTime": "2025-10-22T23:02:53.887Z",
    "endTime": "2025-10-22T23:03:33.909Z",
    "durationMs": 40022,
    "durationSeconds": 40.02
  },
  "stats": {
    "segments": 372,
    "words": 2781,
    "model": "base",
    "device": "cuda"
  }
}
```

---

## ğŸ“Š DivisÃ£o de Responsabilidades

### ORQUESTRADOR (api-gpu)
- âœ… AutenticaÃ§Ã£o (API Key)
- âœ… ValidaÃ§Ã£o de input
- âœ… ComunicaÃ§Ã£o com RunPod API
- âœ… FormataÃ§Ã£o de arquivos (SRT, VTT, ASS, JSON)
- âœ… Upload para S3/MinIO
- âœ… ConstruÃ§Ã£o de response final
- âœ… MÃ©tricas e logs
- âŒ **NÃƒO** faz transcriÃ§Ã£o

### WORKER (whisper-hub ou faster-whisper)
- âœ… Download de Ã¡udio
- âœ… TranscriÃ§Ã£o com Whisper
- âœ… ExtraÃ§Ã£o de segments e word_timestamps
- âœ… DetecÃ§Ã£o de idioma
- âœ… GestÃ£o de GPU/modelo
- âŒ **NÃƒO** formata arquivos (SRT, ASS, etc)
- âŒ **NÃƒO** faz upload para S3
- âŒ **NÃƒO** sabe sobre MinIO

### HUB (RunPod Hub)
- âœ… Armazena imagem Docker prÃ©-built
- âœ… Versionamento de releases
- âœ… Testes automÃ¡ticos
- âœ… Cache para cold start rÃ¡pido
- âŒ **NÃƒO** executa cÃ³digo (apenas armazena)

---

## ğŸ”‘ VariÃ¡veis de Ambiente

### Orquestrador (.env)
```bash
# RunPod API
RUNPOD_API_KEY=rpa_xxx

# Faster-Whisper (atual)
RUNPOD_WHISPER_ENDPOINT_ID=82jjrwujznxwvn

# OpenAI Whisper Official (novo)
RUNPOD_WHISPER_OFFICIAL_ENDPOINT_ID=xxx

# S3/MinIO
S3_ENDPOINT=http://minio.automear.com
S3_ACCESS_KEY=xxx
S3_SECRET_KEY=xxx
S3_BUCKET=canais

# API
X_API_KEY=sua-chave-api
PORT=3000
```

---

## ğŸš€ Dois Endpoints Side-by-Side

### Endpoint 1: Faster-Whisper (Atual)
- **Rota:** `/runpod/audio/transcribe`
- **Worker:** RunPod Hub `runpod-workers/worker-faster_whisper`
- **Endpoint ID:** `82jjrwujznxwvn`
- **Engine:** faster-whisper (CTranslate2)
- **Cold start:** ~6s (hub cacheado)
- **Velocidade:** Muito rÃ¡pida
- **Qualidade:** Excelente

### Endpoint 2: OpenAI Whisper Official (Novo)
- **Rota:** `/runpod/audio/transcribe-whisper`
- **Worker:** Nosso Hub `FresHHerB/whisper-hub`
- **Endpoint ID:** A ser criado
- **Engine:** OpenAI Whisper (PyTorch)
- **Cold start:** ~30s-2min (apÃ³s hub publicado: ~6s)
- **Velocidade:** RÃ¡pida
- **Qualidade:** MÃ¡xima (sem quantizaÃ§Ã£o)

**Ambos compartilham:**
- Mesmo orquestrador (api-gpu)
- Mesma lÃ³gica de formataÃ§Ã£o (TranscriptionFormatter)
- Mesmo upload S3 (S3UploadService)
- Mesmo formato de output

**DiferenÃ§a:**
- Apenas qual worker RunPod chama
- Apenas o `RUNPOD_WHISPER_ENDPOINT_ID` usado

---

## ğŸ¯ Por Que Usar RunPod Hub?

### Problema: Cold Start Lento
Sem hub, cada cold start precisa:
1. Iniciar container vazio
2. Pull da imagem Docker (~8GB)
3. Instalar dependÃªncias
4. Download de modelos Whisper (~4-5GB)

**Tempo total:** 8+ minutos

### SoluÃ§Ã£o: RunPod Hub
Com hub, cold start apenas:
1. Pull imagem prÃ©-built (cache no RunPod)
2. Modelos jÃ¡ estÃ£o na imagem

**Tempo total:** ~6 segundos (similar a faster-whisper)

### BenefÃ­cios do Hub
- âœ… **Cold start 80x mais rÃ¡pido** (6s vs 8min)
- âœ… **Modelos prÃ©-cacheados** (base, medium, turbo)
- âœ… **Versionamento** (v1.0.0, v1.1.0, etc)
- âœ… **Testes automÃ¡ticos** antes de publicar
- âœ… **Marketplace pÃºblico** (outros podem usar)
- âœ… **Badge no README** (credibilidade)

---

## ğŸ“ Estrutura de Arquivos

```
api-gpu/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ orchestrator/
â”‚   â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â”‚   â””â”€â”€ transcription.ts         # Rotas /runpod/audio/*
â”‚   â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”‚   â”œâ”€â”€ runpodWhisperService.ts  # faster-whisper service
â”‚   â”‚   â”‚   â”œâ”€â”€ transcriptionFormatter.ts # Formata SRT/ASS/JSON
â”‚   â”‚   â”‚   â””â”€â”€ s3Upload.ts              # Upload MinIO/S3
â”‚   â”‚   â””â”€â”€ index.ts                     # Registra rotas
â”‚   â””â”€â”€ shared/
â”‚       â””â”€â”€ types/
â”‚           â””â”€â”€ index.ts                  # TypeScript interfaces
â””â”€â”€ docs/
    â”œâ”€â”€ API.md                            # DocumentaÃ§Ã£o de API
    â””â”€â”€ WHISPER_ARCHITECTURE.md           # Este arquivo

whisper-hub/
â”œâ”€â”€ .runpod/
â”‚   â”œâ”€â”€ hub.json                          # ConfiguraÃ§Ã£o RunPod Hub
â”‚   â””â”€â”€ tests.json                        # Testes automÃ¡ticos
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ handler.py                        # RunPod serverless handler
â”‚   â””â”€â”€ predict.py                        # Whisper predictor
â”œâ”€â”€ builder/
â”‚   â”œâ”€â”€ fetch_models.py                   # Pre-download models
â”‚   â””â”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile                            # CUDA + PyTorch + Whisper
â”œâ”€â”€ requirements.txt                      # Runtime deps
â””â”€â”€ README.md                             # DocumentaÃ§Ã£o
```

---

## ğŸ”„ PrÃ³ximos Passos

1. âœ… Arquivos do hub criados (.runpod/hub.json, tests.json)
2. âœ… Git commit + push
3. âœ… Tag v1.0.0 criada
4. âœ… GitHub release publicada
5. â³ **Conectar repositÃ³rio ao RunPod Hub**
6. â³ **Publicar release no hub**
7. â³ **Criar endpoint no RunPod**
8. â³ **Criar `/runpod/audio/transcribe-whisper` no orquestrador**
9. â³ **Testar novo endpoint**

---

## ğŸ“ Suporte

- **DocumentaÃ§Ã£o RunPod Hub:** https://docs.runpod.io/serverless/workers/hub
- **RepositÃ³rio whisper-hub:** https://github.com/FresHHerB/whisper-hub
- **API Documentation:** D:\code\github\api-gpu\docs\API.md
